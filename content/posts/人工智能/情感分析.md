---
title: "情感分析"
date: 2023-06-20T07:11:27+08:00
draft: false
tags: ["人工智能", "python", "情感分析"]
categories: ["人工智能"]
series: ["机器学习"]
series_weight: 1
featuredImage: https://pan.lmio.xyz/pic/cfefde486f01a1bec90369c2b19c7cc1.png
toc:
  enable: true
---

## 系统介绍

#### 背景：

情感分析是一种通过计算机技术来识别和理解文本中的情感倾向和情感状态的方法。随着社交媒体和在线评论的普及，大量的文本数据被生成和分享，这些数据中蕴含着丰富的情感信息。因此，情感分析成为了一个重要的研究领域，并被广泛应用于各种领域，如市场营销、舆情分析、用户反馈分析等。

### 简介

本项目旨在开发一个基于Bert的情感分析系统，通过对输入文本进行情感分析，帮助用户更好地理解和分析文本中的情感信息。通过该系统，用户可以快速准确地获取文本的情感倾向和情感状态，从而支持决策和分析工作。

项目还提供网页和网络接口，方便用户的同时可以更加容易地嵌入代码。

### 使用模型-bert-base-uncased-emotion

我选择了HuggingFace开源的[预训练模型](https://huggingface.co/bhadresh-savani/bert-base-uncased-emotion?text=Su+Yang+is+an+interesting+guy)

该模型在Twitter情感数据集上进行了性能比较。该数据集包含了来自Twitter的文本数据，每个文本都标注了情感类别。我们使用该模型对数据集中的文本进行情感分析，并与其他情感分析模型进行了比较。
![image.png](https://pan.lmio.xyz/pic/cfefde486f01a1bec90369c2b19c7cc1.png)

- 与其他同类模型的对比
![image.png](https://pan.lmio.xyz/pic/e10d8548ac84699b56714ac7c1f6f502.png)
- 学习率：2e-5
- 批量大小：64  
- 训练轮数：8

### 输入输出定义

输入定义：输入是一个包含任意文本的字符串。

输出定义：输出是一个包含情感分析结果的字典，其中包括以下字段的定义：

1. label：表示情感类别的字符串。它指示了对输入文本的情感分类结果，例如'positive'（积极的）、`negative`（消极的）、`neutral`（中性的）等, score:情感占的比重。
2. 'score'：表示情感类别的置信度得分的浮点数。它表示模型对情感分类结果的置信程度，范围通常在0到1之间。在给定的示例中，情感类别为'surprise'的得分为0.0004197491507511586，表示模型对该分类的置信度较低。

## 核心技术介绍：

本项目的核心技术是基于预训练模型Bert的情感分析。Bert是一种基于Transformer的双向编码器架构，通过掩码语言建模（Mask Language Modeling，MLM）目标进行训练。在本项目中，我们使用了经过微调的bert-base-uncased模型，该模型是在情感数据集上进行训练的，训练参数如下：

Bert（Bidirectional Encoder Representations from Transformers）是一种基于Transformer的双向编码器架构，它通过掩码语言建模（Masked Language Modeling，MLM）目标进行训练。

Bert的主要思想是利用双向上下文信息来建模文本的表示。传统的语言模型（如循环神经网络）在生成下一个词时只能依赖前面的词，而Bert通过使用Transformer的自注意力机制，能够同时考虑上下文的信息，从而更好地捕捉词之间的关系。

在Bert的训练过程中，使用了两个任务来训练模型：掩码语言建模和下一句预测任务。掩码语言建模是指将输入文本中的某些词随机掩盖，并让模型预测被掩盖的词。这个任务可以使模型学会根据上下文来预测缺失的词，从而提高模型对语境的理解能力。下一句预测任务是指给定两个句子，模型需要判断它们是否是连续的句子。这个任务可以帮助模型学习句子级别的语义关系。

Bert的编码器由多个Transformer编码器堆叠而成。每个编码器由多头自注意力机制和前馈神经网络组成。自注意力机制可以根据输入序列的不同位置之间的关系来分配不同的注意力权重，从而更好地捕捉上下文信息。前馈神经网络则用于对每个位置的特征进行非线性变换。

在训练过程中，Bert使用了大规模的无标签文本数据，通过预训练来学习通用的语言表示。预训练完成后，可以通过微调（fine-tuning）的方式在特定任务上进行进一步训练，以适应具体的应用场景。

Bert的优点在于它能够通过双向建模来捕捉上下文信息，具有较好的语境理解能力。它在多个自然语言处理任务上取得了显著的性能提升，并成为了自然语言处理领域的重要基准模型之一。

## 缺陷

模型是由大量英文数据进行训练而成，对中文情感的分析并不理想。

为了简单的解决这个问题，我将 HuggingFace 上的两个模型组合在了一起，对输入的中文数据，先使用翻译模型对中文进行翻译，然后使用情感分析模型。

虽然看似解决了问题，但是给服务器增加了负担，也给最终的输出带来了很多的不确定性。

未来可能通过更多中文数据集的训练慢慢优化这个模型，以达到预期的效果。